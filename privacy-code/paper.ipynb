{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings, sys, random, os, csv\n",
    "from contextlib import redirect_stdout\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('./DP_Random_Forest')\n",
    "from Smooth_Random_Trees import DP_Random_Forest\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from diffprivlib.models import LogisticRegression as DPLR\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate, GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from opendp.smartnoise.synthesizers.quail import QUAILSynthesizer\n",
    "from opendp.smartnoise.synthesizers.pytorch.pytorch_synthesizer import PytorchDPSynthesizer\n",
    "from opendp.smartnoise.synthesizers.preprocessors.preprocessing import GeneralTransformer\n",
    "from opendp.smartnoise.synthesizers.pytorch.nn.patectgan import PATECTGAN\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "\n",
    "myseed = 42\n",
    "np.random.seed(myseed)\n",
    "random.seed(myseed)\n",
    "\n",
    "# create dirs for datastes and figures\n",
    "def create_dir_if_not_exists(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "create_dir_if_not_exists('figures')\n",
    "create_dir_if_not_exists('tmp_results')\n",
    "create_dir_if_not_exists('tmp_synthetic_data')\n",
    "create_dir_if_not_exists('results')\n",
    "      \n",
    "# plot is based on the example of diffprivlib, see https://github.com/IBM/differential-privacy-library \n",
    "epsilons = np.logspace(-2, 2, 50)\n",
    "def make_privacy_utility_plot(xlabel, clf_epsilons, clf_accuracies):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.semilogx(clf_epsilons, clf_accuracies, label=xlabel)\n",
    "    plt.xlabel(\"Epsilon\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.axhline(y=0.955, color='coral', linestyle='dotted', label=\"Random Forest\")\n",
    "    plt.axhline(y=0.5, color='grey', linestyle='dotted', label=\"Random Classifier\")\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.legend(loc='lower right', frameon=False)\n",
    "    plt.grid(True, alpha=0.15, linestyle='--')\n",
    "    plt.savefig(f\"./figures/{xlabel.replace(' ', '')}.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f7650",
   "metadata": {},
   "source": [
    "## 1. Read data and select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-awareness",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/browsererkennung_features_tranco500.csv\", delimiter=\",\")\n",
    "# filter rows which consist mainly of NaNs\n",
    "df = df.loc[:, df.isnull().sum() < 0.5*df.shape[0]]\n",
    "df = df.fillna(-1)\n",
    "df[\"browser\"] = [0 if val==\"firefox\" else 1 for val in df.browser]\n",
    "\n",
    "# use only URLs with 10 runs\n",
    "counts = df['url'].value_counts()\n",
    "indices = [i for i,v in enumerate(counts) if v == 10]\n",
    "complete = counts[indices].index\n",
    "urls = list(complete)\n",
    "df = df[df['url'].isin(urls)]\n",
    "\n",
    "# filter constant features\n",
    "df = df.loc[:,df.apply(pd.Series.nunique) != 1]\n",
    "groups = df['url'] \n",
    "\n",
    "# select 10 best features\n",
    "target = np.array(df[\"browser\"])\n",
    "df = df.drop([\"run_id\", \"url\", \"browser\"], axis=1)\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "features_df = selector.fit_transform(df, target)\n",
    "\n",
    "# selected features\n",
    "selected_features = selector.get_support(indices=True)\n",
    "plot_features = {}\n",
    "for s_feature in selected_features:\n",
    "    plot_features[df.columns[s_feature]] = selector.scores_[s_feature]\n",
    "plot_features = {feature: score for feature, score in sorted(plot_features.items(), key=lambda x: x[1])}\n",
    "features_df = pd.DataFrame(features_df, columns=df.columns[selector.get_support()])\n",
    "\n",
    "# insert browser at first position\n",
    "features_df.insert(loc=0,column='browser', value=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00d19e",
   "metadata": {},
   "source": [
    "## 2. Check Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8447c",
   "metadata": {},
   "source": [
    "### 2.1 Random Forest without Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_df.drop([\"browser\"], axis=1)\n",
    "y = features_df[\"browser\"]\n",
    "gkf = GroupKFold(n_splits = 10)\n",
    "accuracies_cv = []\n",
    "recalls_chrome_cv = []\n",
    "recalls_ff_cv = []\n",
    "precisions_chrome_cv = []\n",
    "precisions_ff_cv = []\n",
    "\n",
    "# cross validation\n",
    "for train_index, test_index in gkf.split(X, y, groups):\n",
    "    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    pipeline = Pipeline([('scaler', scaler), ('clf', RandomForestClassifier(n_jobs=-1,\n",
    "                                                                            random_state=myseed,\n",
    "                                                                            n_estimators=50))])\n",
    "    pipeline.fit(X_train_cv, y_train_cv)\n",
    "    y_pred = pipeline.predict(X_test_cv)\n",
    "    accuracies_cv.append(accuracy_score(y_test_cv, y_pred))\n",
    "    recalls_chrome_cv.append(recall_score(y_test_cv, y_pred, pos_label=1))\n",
    "    recalls_ff_cv.append(recall_score(y_test_cv, y_pred, pos_label=0))\n",
    "    precisions_chrome_cv.append(precision_score(y_test_cv, y_pred, pos_label=1))\n",
    "    precisions_ff_cv.append(precision_score(y_test_cv, y_pred, pos_label=0))\n",
    "\n",
    "# report results\n",
    "print(\"Accuracy: %0.3f \\t\\t Std: %0.3f\" % (np.mean(accuracies_cv), np.std(accuracies_cv)))            \n",
    "print(\"Precision - FF: %0.3f \\t\\t Std: %0.3f\" % (np.mean(precisions_ff_cv), np.std(precisions_ff_cv)))\n",
    "print(\"Recall - FF: %0.3f \\t\\t Std: %0.3f\" % (np.mean(recalls_ff_cv), np.std(recalls_ff_cv)))\n",
    "print(\"Precision - Chrome: %0.3f \\t Std: %0.3f\" % (np.mean(precisions_chrome_cv),\n",
    "                                                   np.std(precisions_chrome_cv)))\n",
    "print(\"Recall - Chrome: %0.3f \\t\\t Std: %0.3f\\n\" % (np.mean(recalls_chrome_cv), np.std(recalls_chrome_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2005a7",
   "metadata": {},
   "source": [
    "### 2.2 Differentially Private Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633c458",
   "metadata": {},
   "source": [
    "### 2.2.1 Results for Epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b66ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(myseed)\n",
    "random.seed(myseed)\n",
    "X = features_df.drop([\"browser\"], axis=1)\n",
    "y = features_df[\"browser\"]\n",
    "accuracies_cv = []\n",
    "recalls_chrome_cv = []\n",
    "recalls_ff_cv = []\n",
    "precisions_chrome_cv = []\n",
    "precisions_ff_cv = []\n",
    "gkf = GroupKFold(n_splits = 10)\n",
    "\n",
    "# cross validation\n",
    "for train_index, test_index in gkf.split(X, y, groups):\n",
    "    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "    class_labels_train = np.array(y_train_cv).reshape(-1,1)\n",
    "    class_labels_test = np.array(y_test_cv).reshape(-1,1)\n",
    "    X_train_new = np.concatenate(((np.array(class_labels_train).reshape(-1,1)), X_train_cv), axis=1)\n",
    "    X_test_new = np.concatenate(((np.array(class_labels_test).reshape(-1,1)), X_test_cv), axis=1)\n",
    "    with open('dprf_output.txt', 'w') as dprf_file:\n",
    "        with redirect_stdout(dprf_file):\n",
    "            # epsilon is 1 here\n",
    "            forest = DP_Random_Forest(X_train_new, X_test_new, [], 50, 1)\n",
    "    accuracies_cv.append(forest._accuracy)\n",
    "    recalls_chrome_cv.append(recall_score(y_test_cv, forest._predicted_labels, pos_label=1))\n",
    "    recalls_ff_cv.append(recall_score(y_test_cv, forest._predicted_labels, pos_label=0))\n",
    "    precisions_chrome_cv.append(precision_score(y_test_cv, forest._predicted_labels, pos_label=1))\n",
    "    precisions_ff_cv.append(precision_score(y_test_cv, forest._predicted_labels, pos_label=0))\n",
    "\n",
    "# report results\n",
    "print(\"Accuracy: %0.3f \\t\\t Std: %0.3f\" % (np.mean(accuracies_cv), np.std(accuracies_cv)))\n",
    "print(\"Precision - FF: %0.3f \\t\\t Std: %0.3f\" % (np.mean(precisions_ff_cv), np.std(precisions_ff_cv)))\n",
    "print(\"Recall - FF: %0.3f \\t\\t Std: %0.3f\" % (np.mean(recalls_ff_cv), np.std(recalls_ff_cv)))\n",
    "print(\"Precision - Chrome: %0.3f \\t Std: %0.3f\" % (np.mean(precisions_chrome_cv),\n",
    "                                                   np.std(precisions_chrome_cv)))\n",
    "print(\"Recall - Chrome: %0.3f \\t\\t Std: %0.3f\\n\" % (np.mean(recalls_chrome_cv), np.std(recalls_chrome_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0d86c",
   "metadata": {},
   "source": [
    "### 2.2.2 Results for Different Values of Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce2c00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracies_rf = []\n",
    "np.random.seed(myseed)\n",
    "random.seed(myseed)\n",
    "X = features_df.drop([\"browser\"], axis=1)\n",
    "y = features_df[\"browser\"]\n",
    "\n",
    "# calculate evaluation metrics for different privacy budgets\n",
    "for val in epsilons:\n",
    "    accuracies_cv = []\n",
    "    recalls_chrome_cv = []\n",
    "    recalls_ff_cv = []\n",
    "    precisions_chrome_cv = []\n",
    "    precisions_ff_cv = []\n",
    "    gkf = GroupKFold(n_splits = 10)\n",
    "    for train_index, test_index in gkf.split(X, y, groups):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "        class_labels_train = np.array(y_train_cv).reshape(-1,1)\n",
    "        class_labels_test = np.array(y_test_cv).reshape(-1,1)\n",
    "        X_train_new = np.concatenate(((np.array(class_labels_train).reshape(-1,1)), X_train_cv), axis=1)\n",
    "        X_test_new = np.concatenate(((np.array(class_labels_test).reshape(-1,1)), X_test_cv), axis=1)\n",
    "        with open('dprf_output.txt', 'w') as dprf_file:\n",
    "            with redirect_stdout(dprf_file):\n",
    "                # val is epsilon here\n",
    "                forest = DP_Random_Forest(X_train_new, X_test_new, [], 50, val)\n",
    "        accuracies_cv.append(forest._accuracy)\n",
    "    if round(val) in [1, 10, 100]:\n",
    "        print(\"Epsilon: \", val)\n",
    "        print(\"Accuracy: %0.3f \\t Std: %0.3f\\n\" % (np.mean(accuracies_cv), np.std(accuracies_cv)))\n",
    "    accuracies_rf.append(np.mean(accuracies_cv))\n",
    "\n",
    "# plot values for different epsilons\n",
    "make_privacy_utility_plot(\"DP Random Forest\", epsilons, accuracies_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da1069",
   "metadata": {},
   "source": [
    "## 3. Read Dataframe from CSV and Obtain Results\n",
    "To obtain the results on newly generated datframes, execute the below cells containing QUAIL and PATE-CTGAN (4.) and set the use_given_dataset variable to False before executing this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0660f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "use_given_dataset = True\n",
    "tmp_path = \"\"\n",
    "# create dataframes\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, target, test_size=0.3, \n",
    "                                                    stratify=target, random_state=myseed)\n",
    "# use the given datasets in synthetic_data/ or not\n",
    "if not use_given_dataset:\n",
    "    tmp_path = \"tmp_\"\n",
    "\n",
    "result_file_synth_data = f\"./{tmp_path}results/results_synth_data.csv\"\n",
    "result_file_synth_real_data = f\"./{tmp_path}results/results_synth_data_real_data.csv\"\n",
    "\n",
    "if not use_given_dataset:\n",
    "    # delete temporary results files if present\n",
    "    if os.path.isfile(result_file_synth_data):\n",
    "        os.remove(result_file_synth_data)\n",
    "    if os.path.isfile(result_file_synth_real_data):\n",
    "        os.remove(result_file_synth_real_data)\n",
    "\n",
    "# evaluations for each dataset \n",
    "for file in sorted(os.listdir(f\"./{tmp_path}synthetic_data/\")):\n",
    "    print(file, \"\\n\")\n",
    "    mydf = pd.read_csv(f\"./{tmp_path}synthetic_data/\" + file, index_col=0)\n",
    "    y = mydf[\"browser\"]\n",
    "    X = mydf.drop([\"browser\"], axis=1)\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=myseed, shuffle=True)\n",
    "    \n",
    "    # cross validation only on synthetic data\n",
    "    accuracies_cv = []\n",
    "    recalls_chrome_cv = []\n",
    "    recalls_ff_cv = []\n",
    "    precisions_chrome_cv = []\n",
    "    precisions_ff_cv = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), \n",
    "                             ('rfc', RandomForestClassifier(n_jobs=-1,random_state=myseed, \n",
    "                                                            n_estimators=50))])\n",
    "        pipeline.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = pipeline.predict(X_test_cv)\n",
    "        accuracies_cv.append(accuracy_score(y_test_cv, y_pred))\n",
    "        recalls_chrome_cv.append(recall_score(y_test_cv, y_pred, pos_label=1))\n",
    "        recalls_ff_cv.append(recall_score(y_test_cv, y_pred, pos_label=0))\n",
    "        precisions_chrome_cv.append(precision_score(y_test_cv, y_pred, pos_label=1))\n",
    "        precisions_ff_cv.append(precision_score(y_test_cv, y_pred, pos_label=0))\n",
    "    \n",
    "    # write results to file\n",
    "    file_exists = os.path.isfile(result_file_synth_data)\n",
    "    with open(result_file_synth_data, \"a\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Accuracy\", \"Acc. StdDev\", \n",
    "                             \"Precision Chrome\", \"Prec. Chr. StdDev\", \n",
    "                             \"Precision Firefox\", \"Prec. FF. StdDev\",\n",
    "                             \"Recall Chrome\", \"Rec. Ch. StdDev\", \n",
    "                             \"Recall Firefox\", \"Rec. FF. StdDev\"])\n",
    "        writer.writerow([\"%0.3f\" % np.mean(accuracies_cv), \"%0.3f\" % np.std(accuracies_cv),\n",
    "                         \"%0.3f\" % np.mean(precisions_chrome_cv), \"%0.3f\" % np.std(precisions_chrome_cv),\n",
    "                         \"%0.3f\" % np.mean(precisions_ff_cv), \"%0.3f\" % np.std(precisions_ff_cv),\n",
    "                         \"%0.3f\" % np.mean(recalls_chrome_cv), \"%0.3f\" % np.std(recalls_chrome_cv),\n",
    "                         \"%0.3f\" % np.mean(recalls_ff_cv), \"%0.3f\" % np.std(recalls_ff_cv)])\n",
    "\n",
    "    # training on synthetic data and evaluation on real data \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    rfc = RandomForestClassifier(n_jobs=-1, random_state=myseed, n_estimators=50)\n",
    "    rfc.fit(X, y)\n",
    "\n",
    "    # test model with real test dataframe\n",
    "    X_test_real = X_test.drop([\"browser\"], axis=1)\n",
    "    X_test_real = scaler.transform(X_test_real)\n",
    "    X_test_real = pd.DataFrame(X_test_real)\n",
    "    y_pred = rfc.predict(X_test_real)\n",
    "    \n",
    "    # write results to file\n",
    "    file_exists = os.path.isfile(result_file_synth_real_data)\n",
    "    with open(result_file_synth_real_data, \"a\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Accuracy\", \"Precision Chrome\", \"Precision Firefox\",\n",
    "                             \"Recall Chrome\", \"Recall Firefox\"])\n",
    "        writer.writerow([\"%0.3f\" % accuracy_score(y_test, y_pred),\n",
    "                         \"%0.3f\" % precision_score(y_test, y_pred, pos_label=1),\n",
    "                         \"%0.3f\" % precision_score(y_test, y_pred, pos_label=0),\n",
    "                         \"%0.3f\" % recall_score(y_test, y_pred, pos_label=1),\n",
    "                         \"%0.3f\" % recall_score(y_test, y_pred, pos_label=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809b684",
   "metadata": {},
   "source": [
    "## 4. Create Synthetic Dataframes: PATE-CTGAN + QUAIL\n",
    "\n",
    "Note: Executing this cell might take some time!\n",
    "\n",
    "This cell generates 10 datasets under the condition that these datasets are somewhat balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e1cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the code is based on the example in the smartnoise-samples repository\n",
    "# https://github.com/opendp/smartnoise-samples/blob/master/whitepaper-demos/5-ml-synthetic-data.ipynb\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, target, test_size=0.3, \n",
    "                                                    stratify=target, random_state=myseed)\n",
    "\n",
    "# differentially private classifier\n",
    "# we clip values to 100, suitable for most features\n",
    "def get_dp_classifier(epsilon):\n",
    "            return DPLR(epsilon=epsilon, data_norm=100)\n",
    "\n",
    "# differentially private synthezizer\n",
    "def get_pate_ctgan_synthezizer(epsilon):\n",
    "    return PytorchDPSynthesizer(epsilon=epsilon, preprocessor=None, gan=PATECTGAN(loss='cross_entropy'))\n",
    "\n",
    "class_balance = 0\n",
    "i = 1\n",
    "list_of_df = []\n",
    "# we create 10 datasets\n",
    "while len(list_of_df) < 10:\n",
    "    class_balance = 0\n",
    "    # ensure that the dataset is somewhat balanced\n",
    "    while class_balance < 0.75:\n",
    "        # use 50% of the privacy budget for the classifier, 50% for the synthezizer\n",
    "        quail_synth = QUAILSynthesizer(2, get_pate_ctgan_synthezizer, get_dp_classifier, 'browser',\n",
    "                                       eps_split=0.5, seed=myseed)\n",
    "        quail_synth.fit(X_train)\n",
    "\n",
    "        # specify size of synthetic data frame as training dataset size\n",
    "        training_data_size = (X_train.shape[0])\n",
    "        synthetic_data = quail_synth.sample(int(training_data_size))\n",
    "        synthetic_dataframe = pd.DataFrame(synthetic_data, columns=X_train.columns)\n",
    "        \n",
    "        # check if dataset is somewhat balanced\n",
    "        class_occurences = synthetic_dataframe.browser.value_counts()\n",
    "        class_balance = min(class_occurences) / max(class_occurences)\n",
    "        print(\"Try Number:\", i)\n",
    "        i+=1\n",
    "        print(\"Ratio between min_instances and max_instances of classes is:\", class_balance)\n",
    "        if class_balance < 0.75:\n",
    "            print(\"...trying again to create a balanced dataset.\\n\")\n",
    "    print(class_occurences, \"\\n\")\n",
    "    list_of_df.append(synthetic_dataframe)\n",
    "    print(f\"so far we have {len(list_of_df)} datasets\")\n",
    "\n",
    "for index, bdf in enumerate(list_of_df):\n",
    "    bdf.to_csv(f\"./tmp_synthetic_data/balanced_df_v{index}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
